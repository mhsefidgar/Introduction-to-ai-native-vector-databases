{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_doc",
   "metadata": {},
   "source": [
    "# ðŸ–¼ï¸ Multi-Modal Magic: Image & Text Search with Weaviate\n",
    "\n",
    "Welcome to the frontier of AI search! In this notebook, we aren't just matching keywords; we are building a system that **sees**. By using the `multi2vec-clip` module, we can project both images and text into the same high-dimensional vector space.\n",
    "\n",
    "### The Goal:\n",
    "1. **Initialize** a Weaviate instance that understands cross-modal relationships.\n",
    "2. **Ingest** raw image data converted to Base64.\n",
    "3. **Search** for images using only a text prompt (Text-to-Image).\n",
    "4. **Search** for images using another image (Image-to-Image similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610385fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, weaviate, json, os, IPython, base64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39dad7",
   "metadata": {},
   "source": [
    "### We first need to start a local instance of Weaviate using Docker.\n",
    "\n",
    "1. This can be done by opening up a terminal in the folder with the provided `docker-compose.yml` file in it and typing:\n",
    "```bash\n",
    "docker compose up\n",
    "```\n",
    "\n",
    "2. Later, in order to bring Weaviate down you can just go into this terminal window and type `Ctrl + C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372be9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to locally hosted Weaviate\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "# Check if the service is live\n",
    "print(\"Is Weaviate ready?\", client.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41fc7d",
   "metadata": {},
   "source": [
    "### Q1: Specify an ML model that can understand both images and text into Weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "doc_clip",
   "metadata": {},
   "source": [
    "### ðŸ§  The CLIP Model\n",
    "To make our database \"multi-modal,\" we use **CLIP (Contrastive Language-Image Pre-training)**. It was trained on millions of image-caption pairs to learn how a picture of a \"golden retriever\" relates to the written words \"golden retriever.\"\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c48b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing schema if it exists to start fresh\n",
    "if client.schema.exists(\"Meme\"):\n",
    "    client.schema.delete_class(\"Meme\")\n",
    "\n",
    "class_obj = {\n",
    "    \"class\": \"Meme\",\n",
    "    \"description\": \"A class to store images and their vectors\",\n",
    "    \"moduleConfig\": {\n",
    "        \"multi2vec-clip\": {\n",
    "            \"imageFields\": [\"image\"]\n",
    "        }\n",
    "    ExternalDependencies: [],\n",
    "    \"vectorizer\": \"multi2vec-clip\",\n",
    "    \"properties\": [\n",
    "        {\"name\": \"image\", \"dataType\": [\"blob\"]},\n",
    "        {\"name\": \"filename\", \"dataType\": [\"string\"]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "client.schema.create_class(class_obj)\n",
    "print(\"Schema class 'Meme' created with CLIP vectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faec5b3",
   "metadata": {},
   "source": [
    "### Q2: Add the provided data to your vector database - feel free to use your own images for this aswell!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "doc_base64",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Storing Blobs\n",
    "Weaviate stores images as **Base64 encoded strings**. During import, Weaviate automatically intercepts these strings, passes them through the CLIP model, and stores the resulting vector alongside the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda39cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with client.batch(batch_size=10) as batch:\n",
    "    for img_file in os.listdir(\"Images/\"):\n",
    "        if img_file.endswith((\".jpg\", \".jpeg\", \".png\", \".JPEG\")):\n",
    "            with open(f\"Images/{img_file}\", \"rb\") as f:\n",
    "                b64_image = base64.b64encode(f.read()).decode('utf-8')\n",
    "            \n",
    "            properties = {\n",
    "                \"image\": b64_image,\n",
    "                \"filename\": img_file\n",
    "            }\n",
    "            batch.add_data_object(properties, \"Meme\")\n",
    "    \n",
    "print(\"All images added and vectorized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805aa799",
   "metadata": {},
   "source": [
    "### Q3. Search the data with a text query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "doc_text2img",
   "metadata": {},
   "source": [
    "### ðŸ” Text-to-Image Search\n",
    "Here, we provide a string like \"a sunset over the mountains.\" Weaviate turns that string into a vector and finds the images whose vectors are mathematically closest (using Cosine Similarity).\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de45f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = \"ocean waves\"\n",
    "\n",
    "res = (\n",
    "    client.query\n",
    "    .get(\"Meme\", [\"filename\", \"image\"])\n",
    "    .with_near_text({\"concepts\": [text_query]})\n",
    "    .with_limit(3)\n",
    "    .do()\n",
    ")\n",
    "\n",
    "print(f\"Found {len(res['data']['Get']['Meme'])} images matching '{text_query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8a77c",
   "metadata": {},
   "source": [
    "**Visualize some of the returned images:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display result 1\n",
    "if res['data']['Get']['Meme']:\n",
    "    img_data = res['data']['Get']['Meme'][0]['image']\n",
    "    IPython.display.display(IPython.display.Image(data=base64.b64decode(img_data), width=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fced12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display result 2\n",
    "if len(res['data']['Get']['Meme']) > 1:\n",
    "    img_data = res['data']['Get']['Meme'][1]['image']\n",
    "    IPython.display.display(IPython.display.Image(data=base64.b64decode(img_data), width=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display result 3\n",
    "if len(res['data']['Get']['Meme']) > 2:\n",
    "    img_data = res['data']['Get']['Meme'][2]['image']\n",
    "    IPython.display.display(IPython.display.Image(data=base64.b64decode(img_data), width=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0d3c4",
   "metadata": {},
   "source": [
    "### Feel free to perform more searches and see if you can explain why specific text search queries return images! \n",
    "\n",
    "#### You've successfully used a vector database to build text-to-image search!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba56a9",
   "metadata": {},
   "source": [
    "### Q4. Search the data with an image query.\n",
    "\n",
    "- **Here we will pass in images that are not in the vector database and search for the most similar images as determined by vector search** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616865b",
   "metadata": {},
   "source": [
    "#### First visualize the image you want to query the database with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = 'TestImages/n03417042_28762.JPEG'\n",
    "IPython.display.Image(filename=test_img_path, width=300) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247cef6",
   "metadata": {},
   "source": [
    "#### Now let's write a query to search using this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_img_path, \"rb\") as f:\n",
    "    test_b64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "imres = (\n",
    "    client.query\n",
    "    .get(\"Meme\", [\"filename\", \"image\"])\n",
    "    .with_near_image({\"image\": test_b64})\n",
    "    .with_limit(3)\n",
    "    .do()\n",
    ")\n",
    "\n",
    "print(\"Image-to-Image search completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed284964",
   "metadata": {},
   "source": [
    "**Visualize the images most similar to the input image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "if imres['data']['Get']['Meme']:\n",
    "    IPython.display.display(IPython.display.Image(data=base64.b64decode(imres['data']['Get']['Meme'][0]['image']), width=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(imres['data']['Get']['Meme']) > 1:\n",
    "    IPython.display.display(IPython.display.Image(data=base64.b64decode(imres['data']['Get']['Meme'][1]['image']), width=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(imres['data']['Get']['Meme']) > 2:\n",
    "    IPython.display.display(IPython.display.Image(data=base64.b64decode(imres['data']['Get']['Meme'][2]['image']), width=300))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
